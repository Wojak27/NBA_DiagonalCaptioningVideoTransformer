{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolwojtulewicz/miniconda3/envs/univl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwojaczek27\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "from main_task_caption import init_model, DATALOADER_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage-One:False, Stage-Two:True\n",
      "Set bert_config.num_hidden_layers: 6.\n",
      "Set visual_config.num_hidden_layers: 6.\n",
      "Set bbx_config.num_hidden_layers: 6.\n",
      "Set cross_config.num_hidden_layers: 6.\n",
      "Set decoder_config.num_decoder_layers: 3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "30522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weights from pretrained model cause errors in UniVL: \n",
      "   size mismatch for visual.embeddings.word_embeddings.weight: copying a param with shape torch.Size([768, 1024]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n",
      "   size mismatch for normalize_video.visual_norm2d.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "   size mismatch for normalize_video.visual_norm2d.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).\n"
     ]
    }
   ],
   "source": [
    "from main_task_caption import Args_Caption\n",
    "\n",
    "\n",
    "args = Args_Caption(features_dir=\"data\", do_eval=False, output_dir=\"Finetuned_models/ourds_nodiag_lang\", export_attention_scores=False, task=\"caption-lang\")\n",
    "model = init_model(args, torch.device(\"cpu\"), 0 , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/karolwojtulewicz/code/NSVA/modules/bert-base-uncased/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from modules.tokenization import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "args.video_feature = pickle.load(open(args.features_path, 'rb'))\n",
    "args.video_bbx_feature = pickle.load(open(args.bbx_features_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader, val_length = DATALOADER_DICT[args.datatype][\"val\"](args, tokenizer,split_type='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_dataloader))\n",
    "input_ids, input_mask, segment_ids, video, video_mask, \\\n",
    "pairs_masked_text, pairs_token_labels, masked_video, video_labels_index,\\\n",
    "pairs_input_caption_ids, pairs_decoder_mask, pairs_output_caption_ids,task_type, bbx, bbx_mask = batch\n",
    "\n",
    "yhat = model(input_ids, segment_ids, input_mask, video.float(), video_mask.float(),\n",
    "                pairs_masked_text=pairs_masked_text, pairs_token_labels=pairs_token_labels,\n",
    "                masked_video=masked_video, video_labels_index=video_labels_index,\n",
    "                input_caption_ids=pairs_input_caption_ids, decoder_mask=pairs_decoder_mask,\n",
    "                output_caption_ids=pairs_output_caption_ids,task_type=task_type, bbx=bbx.float(), bbx_mask=bbx_mask.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.410413 to fit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rnn_torchviz.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "attn_mask = np.array([[[[    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,               -0.,     -0., -10000., -10000., -10000., -10000., -10000.,           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,           -10000., -10000.,     -0.,     -0.,     -0.,     -0.,     -0.,               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,               -0.,     -0.,     -0.,     -0., -10000., -10000., -10000.,           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,           -10000., -10000., -10000., -10000.]]]])\n",
    "attn_output = np.array([[[ 7.4819e-01, -1.7681e+00, -7.4810e-01,  6.6186e-01,\n",
    "          -4.2375e-01, -1.5031e+00],\n",
    "         [ 3.2637e-01, -1.2292e+00, -1.3774e+00,  3.8758e-01,\n",
    "          -1.0174e+00, -3.6512e-01],\n",
    "         [ 9.9983e-01, -1.8030e+00, -5.2177e-01,  4.7865e-02,\n",
    "           1.3298e-03, -4.7057e-01],\n",
    "         [-2.4457e-01,  9.7655e-03, -7.4755e-01,  2.1563e+00,\n",
    "           1.7433e+00,  7.5925e-02],\n",
    "         [-6.6065e-01, -9.9056e-01, -2.5342e-01,  1.2676e+00,\n",
    "           1.1478e+00,  1.4212e+00],\n",
    "         [ 1.4926e-01, -6.7882e-01, -1.2417e+00,  1.9228e+00,\n",
    "           1.4427e+00,  1.1168e+00]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 60)\n",
      "(1, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "print(attn_mask.shape)\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "univl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
